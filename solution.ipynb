{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCWwIIzk_W-V"
      },
      "source": [
        "# Generate a Model to Classify Sentiment of a Review Text\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLV_Dtl2_ks7"
      },
      "source": [
        "Congratulations! You're a movie director, and you just released a film you poured your soul (and money) into making. You are curious as to what people thought of your movie. Reviews are pouring in every day, hundreds upon hundreds. You want to read through them all, but you physically can't.\n",
        "\n",
        "On top of being a director, you are also a machine learning programmer, and you think it would be cool to make a model that can look at the text of any review and classify whether it is positive or negative, just by looking at the words. \n",
        "\n",
        "You find a giant dataset of IMDB reviews for a variety of movies, tagged for having `positive` (1) or `negative` (0) sentiments, depending on the rating. \n",
        "\n",
        "With this IMDB dataset, you decided to build and train an AI `Sentiment Analysis` model that can classify any review as positive or negative. That way, you can run the model on any review or comment that comes in for your movie and quickly tell if it is positive or negative without reading it yourself. \n",
        "\n",
        "(Note: This model is called a `Naive-Bayes Classifier`. This model can actually compute the sentiment for **any** text -- reviews, tweets, comments, etc!)\n",
        "\n",
        "(Another note: This is a difficult task! Do not be discouraged if you struggle, and don't be scared to ask for help.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xzn9mL_b8RfA"
      },
      "source": [
        "## Import Libraries (Do not edit) \n",
        "\n",
        "Load python packages necessary for building the sentiment analysis model download the IMDB dataset. Do not edit this cell. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gp8stAtk8I1D",
        "outputId": "85d6a37b-3207-4b49-c758-e743aee62c11"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive # file\n",
        "import string # tokenize\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import matplotlib.pyplot as plt # confusion matrix\n",
        "import math\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split # model\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# for testing later\n",
        "! gdown --id 1IViQp6-J-uZYOsZY4nw_XLE3y9QXhyIw\n",
        "import testing_aides as tester\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "En0wrljAh3Xl"
      },
      "source": [
        "### What does your data look like? (Do Not Edit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "yOnDiqRQh7et",
        "outputId": "0c89c0d8-0ee2-4f8d-f7ca-63d80b2469ea"
      },
      "outputs": [],
      "source": [
        "# get the data!\n",
        "! gdown --id 1vexo0MMtjPzSv5CONn5t8QH3IKvewedk\n",
        "reviewData = pd.read_csv(\"/content/labeledTrainData.tsv\", delimiter=\"\\t\")\n",
        "reviewData.head() # what does this file look like?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQhbxceMD49P"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In lecture, you learned how to compute the sentiment of a text using `multinomial Naive-Bayes`. Recall you need two different kinds of probability - `prior` and `posterior`. \n",
        "\n",
        "So, there are three parts to writing this code:\n",
        "\n",
        "1.   Calculating **prior probabilities** using the given reviews (the \"train data\") **(Part 1)**\n",
        "2.   Creating methods to calculate **posterior probabilities** of individual words in a review **(Part 2)**\n",
        "3.   Classifying each review in the \"test data\" as positive or negative based on the combined prior and posterior probabilities **(Part 3)**\n",
        "\n",
        "The goal is to write code that classifies each review in a **list of reviews** (`reviews_test`) as negative (0) or positive (1) based on probabilities. The final result should be a **list of sentiments** (`classified_sentiments`), which has one sentiment value for each test review.\n",
        "- Ex. [0, 1, 1, 0, 1...] \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sgv-kHhxYk0B"
      },
      "source": [
        "### Variables to Know\n",
        "These are provided variables that you will have to use to develop your code! Know them well.\n",
        "*   `sentimentClasses` is a list of the sentiments, `0` (which represents negative) and `1` (which represents positive).\n",
        "  * `sentimentClasses = [0,1]`\n",
        "  * We will later *classify* each test review into the positive or negative class.\n",
        "  \n",
        "*   `reviews` is a list of all the **review texts**. \n",
        "  * This is the *review* column of your data table.\n",
        "  * Ex. `[\"This was a great movie\", \"Wow! What a flop!\", ...]`\n",
        "*   `sentiment` is a list of the **sentiments** (0 or 1), corresponding to each review from `reviews`. This is the *sentiment* column of the data file.\n",
        "  *  This is the *sentiment* column of your data table\n",
        "  * Ex. `[1, 0, 0, 1, ...]`\n",
        "\n",
        "`reviews[0]` corresponds to `sentiments[0]`, and so on.\n",
        "* Ex. `\"This was a great movie\"` corresponds to `1` (positive)\n",
        "\n",
        "**Note:** The cell below defines these variables for you. You do not need to define them yourself.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyFzSY6m-jP_",
        "outputId": "50d18962-dd8a-4bde-8a1f-270422406ae8"
      },
      "outputs": [],
      "source": [
        "# Provided variables (DO NOT EDIT!)\n",
        "\n",
        "# reviews is the list of reviews\n",
        "reviews = reviewData['review'].to_numpy()   # from 'review' column\n",
        "print(reviews)\n",
        "print(type(reviews))\n",
        "print(len(reviews))\n",
        "# sentiments is the list of corresponding sentiments\n",
        "sentiments = reviewData['sentiment'].values   # from 'sentiment' column\n",
        "print(sentiments)\n",
        "# split reviews and sentiments for training and testing\n",
        "reviews_train, reviews_test, sentiments_train, sentiments_test = train_test_split(reviews, sentiments, test_size=0.99, random_state=RANDOM_SEED)\n",
        "\n",
        "# store the two unique sentiments, 0 (negative) and 1 (positive) in a sentimentClasses\n",
        "sentimentClasses = np.unique(sentiments) # [0, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJB3k_hF_kpw"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Part 1: Calculate Prior Probabilities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdqp5z6jSzJi"
      },
      "source": [
        "\n",
        "### Group Reviews by Sentiment Class\n",
        "First, it's useful to group all the reviews in the training dataset by sentiment. \n",
        "\n",
        "Say you do this by creating one list of all the positive reviews, and one list of all the negative reviews. We can store these lists in a **dictionary**. \n",
        "\n",
        "We will use dictionaries in this lab to organize our data into our two sentiment classes, 0 and 1.\n",
        "\n",
        "This is what the dictionary `ReviewsByClass` should look like:\n",
        "- Keys: the sentiment classes, 0 and 1\n",
        "- Values: lists of all the negative reviews, and all the positive reviews (respectively)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FF4OQKOYLw0"
      },
      "source": [
        "*Example*: Hypothetically, say your reviews list was: \n",
        "  > `[\"Good acting.\", \"I love this film!\", \"Truly horrible. What a flop.\"]`\n",
        "\n",
        "...and your sentiments list was:\n",
        "  > `[1, 1, 0]`\n",
        "\n",
        "`ReviewsByClass` should look something like:\n",
        "  > `{0: [\"Truly horrible. What a flop.\"], 1: [\"Good acting.\", \"I love this film!\"]}`\n",
        "\n",
        "  - Dictionaries have the format `{key: value, key: value, ...}`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkU8xyNhV0nN",
        "outputId": "44712ded-dc53-4129-b101-94fadec22bf2"
      },
      "outputs": [],
      "source": [
        "# TO DO: fill in ReviewsByClass\n",
        "# ReviewsByClass should have two keys, 0 and 1 (one for each sentiment class).\n",
        "# the value for 0 should be a list of all the negative reviews\n",
        "# the value for 1 should be a list of all the positive reviews\n",
        "\n",
        "# first step: go through all the reviews. for each review, sort them into either\n",
        "# negative or positive list based on what its corresponding sentiment is.\n",
        "\n",
        "ReviewsByClass = {} \n",
        "\n",
        "for c in sentimentClasses: # for each class\n",
        "  ReviewList = [] # list of all reviews under class\n",
        "  for i in range(len(reviews)): # go through all reviews\n",
        "    if (sentiments[i] == c): # if classes match\n",
        "      ReviewList.append(reviews[i]) # append that review to the list\n",
        "  ReviewsByClass[c] = ReviewList # list is the value, class is key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni56G76pSV-S"
      },
      "source": [
        "### Calculate the Prior Probabilities\n",
        "Complete a `dictionary` called `PriorProbabilityByClass`.\n",
        "- Keys: the sentiment classes, 0 and 1\n",
        "- Values: the prior probabilities\n",
        "  - prior probability: the probability that **any** review is in a particular sentiment class\n",
        "\n",
        "**Note:** **Prior probability of a class = log((# of items in that class) / (total # of items))**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOTAfIcZjJe0"
      },
      "source": [
        "*Example*: Hypothetically, say your reviews list was: \n",
        "  > `[\"Good acting.\", \"I love this film!\", \"Truly horrible. What a flop.\"]`\n",
        "\n",
        "...and your sentiments list was:\n",
        "  > `[1, 1, 0]`\n",
        "\n",
        "There is 1 negative review, and 2 positive reviews. In total, there are 3 reviews. \n",
        "\n",
        "Therefore, the negative prior probability is `1/3`, and the positive prior  probability is `2/3`.\n",
        "\n",
        "Your final dictionary, `PriorProbabilityByClass`, for this example dataset should look something like:\n",
        "  > `{0: 0.33, 1: 0.67}`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMukh4GuI3kE",
        "outputId": "5fb457dd-dd7a-4a92-8617-93687f8527e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: -0.6931471805599453, 1: -0.6931471805599453}\n"
          ]
        }
      ],
      "source": [
        "# TO DO: fill in PriorProbabilityByClass\n",
        "# PriorProbabilityByClass also has two keys, one for each sentiment class.\n",
        "# the values for each key should be the prior probability for each corresponding class.\n",
        "# ^ a decimal value -> log(# of items in that class / total # of items)\n",
        "\n",
        "# Hint: can you use ReviewsByClass for this?\n",
        "\n",
        "PriorProbabilityByClass = {}\n",
        "\n",
        "totalRevCount = len(reviews) # total # of reviews\n",
        "for c in sentimentClasses:\n",
        "  classRevCount = len(ReviewsByClass[c]) # compute # of reviews in P & N \n",
        "  PriorProbabilityByClass[c] = math.log(classRevCount/totalRevCount) # probability is value, class is key\n",
        "print(PriorProbabilityByClass)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIGiFhrRI-2a"
      },
      "source": [
        "## Part 2: Create Methods to Calculate Posterior Probabilities\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpqzaeuGTI3m"
      },
      "source": [
        "To write the classification code, you will need some **helper methods and variables** to help calculate the posterior probabilities. Build them by following the steps below. Each step depends on the last! \n",
        "\n",
        "*Note*: Additional information for each of the steps is above their code cells.\n",
        "Testing cells are provided after each step so you can check your progress.\n",
        "\n",
        "1. Write a method to `tokenize` a review\n",
        "2. Create the dictionary `BagOfWordsByClass`, which holds a **Bag of Words** dictionary for each sentiment class\n",
        "3. Write a method `computePosteriorProbs(word, c, reviewsByClass, bagOfWords)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrH3mp7WsR29"
      },
      "source": [
        "### Task 1: Tokenize (preprocessing)\n",
        "Write a method to `tokenize` a review.*\n",
        "  - You should do **preprocessing** on the review (this means getting rid of anything from the text that won't be useful for the analysis)\n",
        "    - Make the text lowercase\n",
        "    - Strip the text of any invalid characters (like `\\`)\n",
        "  - Finally, split the review into a list of individual words, and **return** that list.\n",
        "    - You can do this by splitting by punctuation, spaces...\n",
        "\n",
        "**Hint**: the Python **lower(), strip(), replace(), split()** methods\n",
        "\n",
        "*Making this a method is useful later on, because you will need to tokenize a LOT of reviews.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdzoO-PTbGl_"
      },
      "source": [
        "*Example*: \n",
        "\n",
        "`result = tokenize(\"Yikes! That movie stunk a \\lot\\.\")`\n",
        "\n",
        "`result` should be something like `[\"yikes\", \"that\", \"movie\", \"stunk\", \"a\", \"lot\"]`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ETxVqKnyLj--"
      },
      "outputs": [],
      "source": [
        "# input: a review\n",
        "# RETURN: a list of preprocessed words\n",
        "def tokenize(review):\n",
        "    # Preprocessing (any order):\n",
        "    # - make the text lowercase\n",
        "    # - strip the text of any unneccessary characters\n",
        "    # - split the text into a list of words\n",
        "\n",
        "    review = review.lower() # lowercase it\n",
        "    review = review.split(\" \") # split by spaces\n",
        "    #print(review)\n",
        "    finalRev = []\n",
        "    for rev in review:\n",
        "       rev = rev.strip() # strip of chars at beginning and end\n",
        "       rev = rev.replace('\\\\', '') # get rid of weird chars\n",
        "       rev = rev.replace('<', '')\n",
        "       rev = rev.replace('>', '')\n",
        "       rev = rev.replace('.', '') # ...\n",
        "       finalRev.append(rev)\n",
        "\n",
        "    return finalRev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4QMwquWOeQ7",
        "outputId": "975ccbc0-22fd-4fe3-9c68-9c79da79c668"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', \"i've\", 'started', 'listening', 'to', 'his', 'music,', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there,', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again', 'maybe', 'i', 'just', 'want', 'to', 'get', 'a', 'certain', 'insight', 'into', 'this', 'guy', 'who', 'i', 'thought', 'was', 'really', 'cool', 'in', 'the', 'eighties', 'just', 'to', 'maybe', 'make', 'up', 'my', 'mind', 'whether', 'he', 'is', 'guilty', 'or', 'innocent', 'moonwalker', 'is', 'part', 'biography,', 'part', 'feature', 'film', 'which', 'i', 'remember', 'going', 'to', 'see', 'at', 'the', 'cinema', 'when', 'it', 'was', 'originally', 'released', 'some', 'of', 'it', 'has', 'subtle', 'messages', 'about', \"mj's\", 'feeling', 'towards', 'the', 'press', 'and', 'also', 'the', 'obvious', 'message', 'of', 'drugs', 'are', 'bad', \"m'kaybr\", '/br', '/visually', 'impressive', 'but', 'of', 'course', 'this', 'is', 'all', 'about', 'michael', 'jackson', 'so', 'unless', 'you', 'remotely', 'like', 'mj', 'in', 'anyway', 'then', 'you', 'are', 'going', 'to', 'hate', 'this', 'and', 'find', 'it', 'boring', 'some', 'may', 'call', 'mj', 'an', 'egotist', 'for', 'consenting', 'to', 'the', 'making', 'of', 'this', 'movie', 'but', 'mj', 'and', 'most', 'of', 'his', 'fans', 'would', 'say', 'that', 'he', 'made', 'it', 'for', 'the', 'fans', 'which', 'if', 'true', 'is', 'really', 'nice', 'of', 'himbr', '/br', '/the', 'actual', 'feature', 'film', 'bit', 'when', 'it', 'finally', 'starts', 'is', 'only', 'on', 'for', '20', 'minutes', 'or', 'so', 'excluding', 'the', 'smooth', 'criminal', 'sequence', 'and', 'joe', 'pesci', 'is', 'convincing', 'as', 'a', 'psychopathic', 'all', 'powerful', 'drug', 'lord', 'why', 'he', 'wants', 'mj', 'dead', 'so', 'bad', 'is', 'beyond', 'me', 'because', 'mj', 'overheard', 'his', 'plans?', 'nah,', 'joe', \"pesci's\", 'character', 'ranted', 'that', 'he', 'wanted', 'people', 'to', 'know', 'it', 'is', 'he', 'who', 'is', 'supplying', 'drugs', 'etc', 'so', 'i', 'dunno,', 'maybe', 'he', 'just', 'hates', \"mj's\", 'musicbr', '/br', '/lots', 'of', 'cool', 'things', 'in', 'this', 'like', 'mj', 'turning', 'into', 'a', 'car', 'and', 'a', 'robot', 'and', 'the', 'whole', 'speed', 'demon', 'sequence', 'also,', 'the', 'director', 'must', 'have', 'had', 'the', 'patience', 'of', 'a', 'saint', 'when', 'it', 'came', 'to', 'filming', 'the', 'kiddy', 'bad', 'sequence', 'as', 'usually', 'directors', 'hate', 'working', 'with', 'one', 'kid', 'let', 'alone', 'a', 'whole', 'bunch', 'of', 'them', 'performing', 'a', 'complex', 'dance', 'scenebr', '/br', '/bottom', 'line,', 'this', 'movie', 'is', 'for', 'people', 'who', 'like', 'mj', 'on', 'one', 'level', 'or', 'another', '(which', 'i', 'think', 'is', 'most', 'people)', 'if', 'not,', 'then', 'stay', 'away', 'it', 'does', 'try', 'and', 'give', 'off', 'a', 'wholesome', 'message', 'and', 'ironically', \"mj's\", 'bestest', 'buddy', 'in', 'this', 'movie', 'is', 'a', 'girl!', 'michael', 'jackson', 'is', 'truly', 'one', 'of', 'the', 'most', 'talented', 'people', 'ever', 'to', 'grace', 'this', 'planet', 'but', 'is', 'he', 'guilty?', 'well,', 'with', 'all', 'the', 'attention', \"i've\", 'gave', 'this', 'subjecthmmm', 'well', 'i', \"don't\", 'know', 'because', 'people', 'can', 'be', 'different', 'behind', 'closed', 'doors,', 'i', 'know', 'this', 'for', 'a', 'fact', 'he', 'is', 'either', 'an', 'extremely', 'nice', 'but', 'stupid', 'guy', 'or', 'one', 'of', 'the', 'most', 'sickest', 'liars', 'i', 'hope', 'he', 'is', 'not', 'the', 'latter']\n",
            "['br', '/br', '/this', 'movie', 'is', 'full', 'of', 'references', 'like', 'mad', 'max', 'ii\",', '\"the', 'wild', 'one\"', 'and', 'many', 'others', 'the', 'ladybug´s', 'face', 'it´s', 'a', 'clear', 'reference', '(or', 'tribute)', 'to', 'peter', 'lorre', 'this', 'movie', 'is', 'a', 'masterpiece', 'we´ll', 'talk', 'much', 'more', 'about', 'in', 'the', 'future\"']\n"
          ]
        }
      ],
      "source": [
        "# testing cell (optional): tokenize a review using your method!\n",
        "# the output should be a list of plain words.\n",
        "# suggestions to tokenize:\n",
        "# - the first review in the data (reviews[0])\n",
        "# - the tenth review in the data (reviews[9])\n",
        "print(tokenize(reviews[0]))\n",
        "\n",
        "print(tokenize(reviews[9]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4OVYHf0P3Ec"
      },
      "source": [
        "### Task 2: Create and store bag of words for each class\n",
        "Create the dictionary `BagOfWordsByClass`, which holds a **Bag of Words** dictionary **for each sentiment class**. \n",
        "\n",
        "\n",
        "Here, you will create **two** Bags of Words, one for 0 (negative class) and one for 1 (positive class) and store them both inside `BagOfWordsByClass`:\n",
        "- Keys: the two sentiment classes\n",
        "- Values: the two corresponding Bag of Words dictionaries\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VSXGJxEfgok"
      },
      "source": [
        "**Bag Of Words** is a dictionary with\n",
        "- Keys: **unique** words in the text\n",
        "- Values: the word **count**; how many of those words seen in the text.\n",
        "  - Here, the \"text\" is a list of reviews\n",
        "\n",
        "\n",
        "*Example*: Given the list `[\"happy birthday\", \"i am happy\", \"who am i\"]`\n",
        "- Bag of Words: `{\"happy\": 2, \"birthday\": 1, \"i\": 2, \"am\": 2, \"who\": 1}`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfrkjGdPgk59"
      },
      "source": [
        "**Hint**: \n",
        "- For each class, go through the reviews, each review word by word (what method have we written to split reviews into words?). \n",
        "- For each word: if the word is already in your Bag Of Words dictionary for that class (as a key), increase the word count (the value of that key). \n",
        "- If the word is NOT in your dictionary already, you will have to **add it to the dictionary**.\n",
        "\n",
        "\n",
        "How do you check if a word is in your Bag Of Words?\n",
        "\n",
        "  - You can use the keyword `in` to check if a key exists in a dictionary:\n",
        "      - ex. `if \"funny\" in someArbitraryBagOfWords:`\n",
        "\n",
        "  - You can also use `set`s to store all the unique words you've seen so far in a class or overall. \n",
        "    - A `set` is basically a `list` of unique things that don't have an order. \n",
        "    - It's a lot quicker to check if something is in a `set` than if it is in a `list`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lw7l_yPDfm5K"
      },
      "source": [
        "A `set` we have declared for you is called `vocab`, to keep track of all the unique words you see in the dataset, regardless of class. If you see a unique word, add to `vocab`.\n",
        "* `vocab` --> set of **all unique words**\n",
        "  * This is useful later on.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnOIKBKHfkYe"
      },
      "source": [
        "Finally, we provide a **testing aide** for this part, `testBagOfWordsByClass`, which you can use to check your final `BagOfWordsByClass` dictionary against the original dataset. \n",
        "- Don't worry if your dictionary isn't 100% accurate; your `tokenize()` method may just look different from the tester's. This is just a sanity check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Hub7IQ7hLyu_"
      },
      "outputs": [],
      "source": [
        "# create two bag of words dictionaries, one for neg class, one for pos class\n",
        "# for each review in neg class, split it into words\n",
        "# and for each word, if it is new to neg bag of words, add() to neg bag of words, \n",
        "# if not, add to the count in neg bag of words.\n",
        "# same for pos.\n",
        "# note: also fill vocab: add any new word to vocab\n",
        "# finally, store both in BagOfWordsByClass, by class.\n",
        "\n",
        "BagOfWordsByClass = {}\n",
        "vocab = set()   # set of all unique words. add to it as you go along\n",
        "\n",
        "\n",
        "for c in sentimentClasses: # for each class\n",
        "  bagOfWords = {} # class-specific\n",
        "  \n",
        "  for review in ReviewsByClass[c]: # for each review in the class\n",
        "    wordList = tokenize(review) # list of tokenized words from review\n",
        "   \n",
        "    for word in wordList: # for each word\n",
        "\n",
        "      if word not in bagOfWords:\n",
        "        bagOfWords[word] = 1  # initialize count\n",
        "      else:\n",
        "        bagOfWords[word] += 1 # add to count\n",
        "\n",
        "      if word not in vocab: # setting up an overall vocab list for alter\n",
        "        vocab.add(word)\n",
        "  \n",
        "  if c == 0:\n",
        "    BagOfWordsByClass[0] = bagOfWords\n",
        "  else:\n",
        "    BagOfWordsByClass[1] = bagOfWords\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2K1Bmi7VOAH",
        "outputId": "da06c2dd-f1e1-4441-c945-f8b0480f1a87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- in original dataset ---\n",
            "40 occurrences of horribly in positive reviews\n",
            "163 occurrences of horribly in negative reviews\n",
            "--- in BagOfWordsByClass ---\n",
            "41 occurrences of horribly in positive reviews\n",
            "167 occurrences of horribly in negative reviews\n"
          ]
        }
      ],
      "source": [
        "# testing cell (optional): run this cell after completing BagOfWordsByClass.\n",
        "# to see how accurate your bag-of-words dictionary is.\n",
        "# this is working code - you don't have to write your own testing code here.\n",
        "# change the testWord and run the cell again!\n",
        "\n",
        "testWord = \"horribly\"\n",
        "tester.testBagOfWordsByClass(testWord, reviews, sentiments, BagOfWordsByClass)\n",
        "\n",
        "# !! don't worry if the numbers aren't exactly the same !!\n",
        "\n",
        "# IMPORTANT: remember that your bag-of-words uses TOKENIZED words - words that\n",
        "# have gone through whatever preprocessing you wrote in tokenize().\n",
        "# as the words in the original dataset are unprocessed, this may result in inconsistencies.\n",
        "\n",
        "# the code uses its OWN VERSION of tokenize() that lowercases and gets rid of symbols.\n",
        "# so, there still may be inconsistencies in word counts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSP1JFm4VANN"
      },
      "source": [
        "### Task 3: Calculate posterior probabilities\n",
        "3. Write a method `computePosteriorProbs(word, c, reviewsByClass, bagOfWords)`\n",
        "  - This calculates **posterior probability**: the probability of a given **word** being in a given **class**.\n",
        "    - Parameters include:\n",
        "      - `word`, a string, e.g. \"great\"\n",
        "      - `c`, a given class, e.g. 1\n",
        "      - `reviewsByClass`, the dictionary `ReviewsByClass`\n",
        "      - `bagOfWordsByClass`, the dictionary `BagOfWordsByClass`\n",
        "  - `computePosteriorProbs` should return a number, the probability value.\n",
        "\n",
        "**Note: Posterior Probabilty given word and class = log( `# of that word in the class` + 1 / `# of words in the class` + `# of unique words in total`)**\n",
        "\n",
        "\n",
        "**Note 2:** Sometimes, a word will be in one class but not the other. (Ex. \"fantastic\" might have been seen in the positive class, but not the negative class). \n",
        "- So, you must check whether the inputted `word` is in the Bag of Words for the inputted `class`.\n",
        "  - If it isn't, you will have to **add that word to the bag of words for that class with the initial count `0`**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "uW5gazP3PRcr"
      },
      "outputs": [],
      "source": [
        "# helper vars for computePosteriorProbs\n",
        "# calculating these inside the function takes too long \n",
        "negWordCounts = BagOfWordsByClass[0].values()\n",
        "posWordCounts = BagOfWordsByClass[1].values()\n",
        "\n",
        "negWordSum = sum(negWordCounts)\n",
        "posWordSum = sum(posWordCounts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "haMWYH8dL3gF"
      },
      "outputs": [],
      "source": [
        "# write a method to calculate posterior probability\n",
        "# given a word and a class; the probability that the word will be in the class.\n",
        "# RETURN the probability value\n",
        "def computePosteriorProbs(word, c, reviewsByClass, bagOfWordsByClass):\n",
        "  # account for KeyError\n",
        "  if c == 0:\n",
        "    if word not in bagOfWordsByClass[0]:\n",
        "      bagOfWordsByClass[c][word] = 0\n",
        "  else:\n",
        "    if word not in bagOfWordsByClass[1]:\n",
        "      bagOfWordsByClass[c][word] = 0\n",
        "\n",
        "  num = bagOfWordsByClass[c][word] + 1\n",
        "\n",
        "  # get total number of words in class\n",
        "  if c == 0:\n",
        "    sumWords = negWordSum\n",
        "  else:\n",
        "    sumWords = posWordSum\n",
        "    \n",
        "  denom = sumWords + len(vocab)\n",
        "\n",
        "  return math.log(num/denom)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OalsloHAVNm4",
        "outputId": "f643587f-ae6e-40f9-af6f-c160e8afb744"
      },
      "outputs": [],
      "source": [
        "# testing cell (optional): test computePosteriorProbs()\n",
        "# you can refer to the simple dataset from the lecture as an example of what the\n",
        "# posterior probability should look like.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHZZi238VO9f"
      },
      "source": [
        "## Part 3: Final classification\n",
        "\n",
        "Finally, using the methods and variables you have at your disposal, **write the final classification code** and fill `classified_sentiments`! \n",
        "\n",
        "### Task:\n",
        "For each review in a **list of reviews** (`reviews_test`), classify each review as negative (0) or positive (1) using probabilities. The final result should be a **list of sentiments** (`classified_sentiments`), which has one sentiment value for each test review.\n",
        "- Ex. [0, 1, 1, 0, 1...] \n",
        "\n",
        "\n",
        "### Process:\n",
        "You must classify each review as positive or negative. How do you do this?\n",
        "\n",
        "For each review, you will calculate a **probability score** for each sentiment class; a score for positive sentiment and score for negative sentiment. Then, in the end, you can choose the sentiment with the higher score to be the prediction for that review.\n",
        "\n",
        "Calculating a probability score for a class involves **cumulatively adding posterior probabilities to the prior probability**.\n",
        "\n",
        "- So, for each review, **for each sentiment class**, get the prior probability. - Then, for each word in the review, calculate the posterior probability and add it to the prior probability to get the final probability score.\n",
        "  - **Note**: if the word *hasn't been seen before in the training dataset* (i.e. it is not in the vocab), we don't have any useful information about it, so it's best to skip it entirely.\n",
        "\n",
        "\n",
        "Then, you should choose the sentiment class (0 or 1) with the higher final score for the review. \n",
        "- You can use the Python `max(..., key=...)` function for this.\n",
        "\n",
        "\n",
        "Finally, add that sentiment value to `classified_sentiments`. This is already initialized, so you just need to fill it. You should not write this process as a method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "vGWA---y-Jsh"
      },
      "outputs": [],
      "source": [
        "classified_sentiments = [] # the final list you should store your classified sentiments in\n",
        "\n",
        "# from a list of test reviews  (reviews_test)\n",
        "# for each review, calculate the probability scores for each sentiment class\n",
        "# and choose the sentiment whose score is higher.\n",
        "# calculate these scores by adding posterior probs (word by word) to the prior prob.\n",
        "# add the final classification to classified_sentiments\n",
        "\n",
        "for review in reviews_test: # for each review**** \n",
        "\n",
        "    classProbScores = {} # to use to determine end result\n",
        "\n",
        "    for c in sentimentClasses:\n",
        "      classProbScores[c] = PriorProbabilityByClass[c] # set up with prior probability**\n",
        "      \n",
        "    # split review into words\n",
        "    words = tokenize(review)\n",
        "    \n",
        "    for word in words: # for each word\n",
        "      \n",
        "      if word in vocab: # skip if not\n",
        "\n",
        "        for c in sentimentClasses: # for each class\n",
        "          classProbScores[c] += computePosteriorProbs(word, c, ReviewsByClass, BagOfWordsByClass) # add probs for word to score\n",
        "\n",
        "    # decide which is higher, which class the review belongs in\n",
        "\n",
        "    #classified_sentiments.append(max(classProbScores, key=classProbScores.get))\n",
        "    if classProbScores.get(0) > classProbScores.get(1):\n",
        "      classified_sentiments.append(0)\n",
        "    else:\n",
        "      classified_sentiments.append(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJVnscDToNJL"
      },
      "source": [
        "## Evaluate your model (Do not edit)\n",
        "\n",
        "Now that you've built your classifier and created a final list of sentiments, run the following code cells to evaluate your model for accuracy and other metrics. The following code uses methods from a package called `sklearn`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8RrHkNJM_Kx",
        "outputId": "658125a9-917e-40dd-9cf9-69a5300bdbab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9383030303030303"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# calculate the accuracy of your classification\n",
        "# (sentiments_test contains the correct sentiments)\n",
        "accuracy_score(sentiments_test, classified_sentiments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "xGd0y932Mu_O",
        "outputId": "331be450-d525-4f8f-b001-032391a2faf2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'              precision    recall  f1-score   support\\n\\n           0       0.92      0.96      0.94     12374\\n           1       0.96      0.91      0.94     12376\\n\\n    accuracy                           0.94     24750\\n   macro avg       0.94      0.94      0.94     24750\\nweighted avg       0.94      0.94      0.94     24750\\n'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# calculate performance of the model on several metrics\n",
        "classification_report(sentiments_test, classified_sentiments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Yvkn6uy16w3m"
      },
      "outputs": [],
      "source": [
        "# draw the confusion matrix using matplotlib\n",
        "def drawConfusionMatrix():\n",
        "\n",
        "  # get the data for the confusion matrix\n",
        "  cnf_data = confusion_matrix(sentiments_test, classified_sentiments)\n",
        "\n",
        "  # set up the figure \n",
        "  figure = plt.figure()\n",
        "  axes = figure.add_subplot(111)\n",
        "  \n",
        "  # using the matshow() function\n",
        "  caxes = axes.matshow(cnf_data, cmap='Blues') # cmap -> colors\n",
        "\n",
        "  # title\n",
        "  plt.title('Confusion Matrix For Your Model')\n",
        "\n",
        "  # axis titles\n",
        "  axes.set_xlabel(\"Predicted sentiment\")\n",
        "  axes.set_ylabel(\"Actual sentiment\")\n",
        "\n",
        "  # axis ticks and labels\n",
        "  axes.tick_params(axis=\"x\", bottom=True, top=False, labelbottom=True, labeltop=False)\n",
        "  axes.tick_params(axis=\"y\", left=True, labelleft=True, rotation=90)\n",
        "\n",
        "  labels = ['positive', 'negative']\n",
        "  axes.set_xticklabels(['']+labels)\n",
        "  axes.set_yticklabels(['']+labels)\n",
        "\n",
        "  # inner labels\n",
        "  for (i, j), z in np.ndenumerate(cnf_data):\n",
        "      axes.text(j, i, '{}'.format(z), ha='center', va='center', fontsize=15,\n",
        "                bbox=dict(boxstyle='round', facecolor='white'))\n",
        "\n",
        "  plt.grid(False) # get rid of extra gridlines\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "CR6In4MaGzre",
        "outputId": "0368475c-eb48-453a-d499-f04051a22599"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "drawConfusionMatrix()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ocgFNBUaIyj"
      },
      "source": [
        "## Reflection\n",
        "\n",
        "Try running your classification code on your own made-up reviews! Can you make up a review that the model fails on? (The review is positive but it returns negative, vice versa). \n",
        "\n",
        "You can also try running the method on single words to see what words are considered indicative of positive or negative sentiment.\n",
        "\n",
        "Can you think of any bigger datasets that this model would fail on?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gk0oAc6bbICS"
      },
      "source": [
        "## Additional Activity: Improving Your Model By Improving Preprocessing\n",
        "\n",
        "Look back at the `tokenizing` step of this lab (Part 2). \n",
        "\n",
        "Your `tokenize` method includes **preprocessing** steps like lowercasing the text, getting rid of extraneous symbols, and splitting the review into words. This is all to reduce the raw review text into useful information -- important words.\n",
        "\n",
        "\n",
        "There are really numerous ways to do preprocessing. See if you can improve your `tokenize` method further by employing different methods. \n",
        "\n",
        "- One idea is to try removing words that aren't important for analyzing sentiment. \n",
        "  - For example, `\"I\"` and `\"or\"` aren't really positive or negative (or they shouldn't be!), so they shouldn't have much effect on the final computed sentiment of a review.\n",
        "\n",
        "- **Hint**: The `nltk` python package is specialized for text analysis and it may have some useful methods for this task.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Xzn9mL_b8RfA"
      ],
      "name": "(Solution) Movie Review Sentiment Analysis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
